{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146d1c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#Read in data and set options\n",
    "d = pd.read_excel(\"data.xlsx\", index_col = 0)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "d.columns = [c.replace(' ', '_') for c in d.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4afe7a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "d.clean = d.copy()\n",
    "\n",
    "#Drop duplicate and empty columns\n",
    "d.clean = d.drop(['Location_(Location_Name)', 'Responsibilities_and_Achievements', 'Nationality_(Label)'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb2c833",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create column for whether we have eduaction info\n",
    "d['Highest_Degree'].fillna(\"0\", inplace = True)\n",
    "\n",
    "d.clean['Education'] = d.apply(lambda x: 0 if x[list(d.columns).index('Highest_Degree')] == \"0\" else 1, axis = 1)\n",
    "\n",
    "#Drop other education fields\n",
    "d.clean.drop(['Degree_(Picklist_Label)', 'Did_you_graduate?_(Picklist_Label)', \n",
    "        'Major_(Picklist_Label)', 'Education_Record_is_blank?', 'Highest_Degree', \n",
    "        'Is_Highest_Degree?'], axis=1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c12c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bin marital statuses\n",
    "d.clean['Marital_Status'] = d.apply(lambda x: \"Single\" if x[list(d.columns).index('Marital_Status_(Label)')] == \"Single\" \n",
    "                                    else (\"Married/Living Together\" if x[list(d.columns).index('Marital_Status_(Label)')] in \n",
    "                                    ['Married', 'Domestic Partner', 'Domestic Partner (Unregistered Marriage)', \n",
    "                                    'Living Together', 'Widowed With Surviving Pension'] else (\"Divorced/Widowed\" \n",
    "                                    if x[list(d.columns).index('Marital_Status_(Label)')] in\n",
    "                                    ['Divorced', 'Legally Separated', 'Widowed'] else \"Unknown\")), axis = 1)\n",
    "\n",
    "#Drop original marital status column\n",
    "d.clean.drop('Marital_Status_(Label)', axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca8f6c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop some other irrelevant columns\n",
    "#Chose to drop Job_Category_(Picklist_Label) because it is identical to Job_Category_(Job_Class) with more NA values\n",
    "d.clean.drop(['Rating_Label', 'Time_Type', 'Generation', 'Is_Fulltime_Employee?', \n",
    "              'Frequency', 'Employee_Type', 'Current_Hire_Date', 'Continuous_Service_Date', \n",
    "              'Length_of_Service_-_Buckets','Job_Family', 'Location_Name'\n",
    "              'Comp_Grade_Profile','Currency','Annualized_Mid_Point',\n",
    "              'Pay_Grade', 'Organization_(Label)','Termination_Date'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd7aa70f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop interns - at request of company\n",
    "\n",
    "index = d.clean[d.clean['Employee_Type'] == 'Intern'].index\n",
    "\n",
    "d.clean.drop(index, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a68b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create list of \"remote\" locations\n",
    "\n",
    "df = d.clean.loc[d.clean['Location_Name'].str.startswith('Remote') == True]\n",
    "df = pd.DataFrame(df['Location_Name'].value_counts())\n",
    "remotes = list(df.index)\n",
    "\n",
    "#Create list of locations with very few employees\n",
    "dfNew = pd.DataFrame(d.clean['Location_Name'].value_counts())\n",
    "smalls = list(dfNew[dfNew['Location_Name'] < 50].index)\n",
    "\n",
    "#Create New Location Column - using lists above\n",
    "d.clean['Location'] = d.clean.apply(lambda x: 'Remote' if x[list(d.clean.columns).index('Location_Name')] in remotes \n",
    "                                    else ('Other' if ((x[list(d.clean.columns).index('Location_Name')] not in remotes))\n",
    "                                          & (x[list(d.clean.columns).index('Location_Name')] in smalls) \n",
    "                                          else x[list(d.clean.columns).index('Location_Name')]), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a72d9bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop duplicate rows\n",
    "\n",
    "d.clean.drop_duplicates(inplace = True)\n",
    "d.clean.drop(index = 22613, axis = 0, inplace = True)\n",
    "d.clean.drop(index = 46660, axis = 0, inplace = True)\n",
    "d.clean.drop(index = 26256, axis = 0, inplace = True)\n",
    "\n",
    "#Set GID_anonymized as index\n",
    "\n",
    "d.clean.set_index('GID_anonymized', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05790855",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fill in null values in the compensation grade column using information from other columns\n",
    "\n",
    "for i in range(d.clean.shape[0]):\n",
    "    if ((pd.isna(d.clean['Compensation_Grade'][i])) & (not pd.isna(d.clean['Job_Category'][i])) & (not pd.isna(d.clean['Job_Level'][i]))):\n",
    "        d.clean['Compensation_Grade'][i] = d.clean['Job_Category'][i] + ' ' + str(int(d.clean['Job_Level'][i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98848c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fill null vaues in gender column\n",
    "\n",
    "d.clean['Gender'] = d.clean[['Gender']].fillna('D', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed387be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create dataframe that contains the mean count of direct reports by compensation grade (not including outliers)\n",
    "\n",
    "newDF = d.clean[d.clean['Count_of_Direct_Reports'] > 0]\n",
    "meanReports = pd.DataFrame(round(newDF.groupby('Compensation_Grade').Count_of_Direct_Reports.mean(), 0))\n",
    "meanReports.reset_index(level = 0, inplace = True)\n",
    "\n",
    "#Impute mean values for outliers in direct reports column\n",
    "\n",
    "for i in range(len(d.clean['Count_of_Direct_Reports'])):\n",
    "    if d.clean['Count_of_Direct_Reports'][i] < 0:\n",
    "        comp = d.clean['Compensation_Grade'][i]\n",
    "        d.clean['Count_of_Direct_Reports'][i] = meanReports[meanReports['Compensation_Grade'] == comp]['Count_of_Direct_Reports']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3957272e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bin ethnicities\n",
    "#We realize that binning races together can be potentially color blind but felt we were able to do this in a fair way due to the amazing diversity of our team.\n",
    "\n",
    "\n",
    "d.clean['Ethnicity'] = d.clean.apply(lambda x: \"Native American\" if x[list(d.clean.columns).index('Race/Ethnicity')] in\n",
    "                                     [\"American Indian or Alaskan Native, not Hispanic or Latino\", \"Native Hawaiian or Other Pacific Islander, not Hispanic or Latino\"]\n",
    "                                     else (\"White\" if x[list(d.clean.columns).index('Race/Ethnicity')] in\n",
    "                                          [\"White, not Hispanic or Latino\", \"Caucasian\", \"Eurasian\"] else (\n",
    "                                          \"SE Asian\" if x[list(d.clean.columns).index('Race/Ethnicity')] in \n",
    "                                          [\"Malay\", \"Sarawakian\", \"Vietnamese\", \"Filipino\"] else (\n",
    "                                          \"South Asian\" if x[list(d.clean.columns).index('Race/Ethnicity')] in \n",
    "                                          [\"Indian\", \"Sikh\", \"Pakistani\"] else (\n",
    "                                          \"Other/Unknown\" if x[list(d.clean.columns).index('Race/Ethnicity')] in\n",
    "                                          [\"I do not wish to disclose\", \"Two or More Races, not Hispanic or Latino\", \"others\"] \n",
    "                                          else x[list(d.clean.columns).index('Race/Ethnicity')] )))), axis = 1)\n",
    "d.clean['Ethnicity'] = d.clean[['Ethnicity']].fillna('Other/Unknown', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "025611b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Target Variable\n",
    "#Maps to 1 if event reason name is \"Voluntary Termination\" or if termination reason in \"Job Abandonment\"\n",
    "\n",
    "d.clean['Status'] = d.clean.apply(lambda x: 1 if \n",
    "                                  (x[list(d.clean.columns).index('Event_Reason_Name')] == 'Voluntary Termination') | \n",
    "                                  ((x[list(d.clean.columns).index('Event_Reason_Name')] == 'Involuntary Termination') &\n",
    "                                  (x[list(d.clean.columns).index('Termination_Reason_(externalName)')] == 'Job Abandonment'))\n",
    "                                 else 0, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d21a374",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop more columns\n",
    "\n",
    "d.clean.drop(['Employee_Status', 'Event_Reason_Name', 'Termination_Reason_(externalName)',\n",
    "             'Race/Ethnicity', 'Management_Level_(Picklist_Label)', 'Status'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a27eb65f",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "At this point, we had done everything we could to fill the NAs with the information\n",
    "provided to us by company.  Given time constraints and other limitations, we were \n",
    "unable to fill any more in a reasonable way.  Here we create a new dataframe without\n",
    "any NA values - we did this using .copy() in case this data could be provided later.\n",
    "'''\n",
    "\n",
    "d.clean.NoNa = d.clean.copy()\n",
    "d.clean.NoNa.dropna(inplace = True)\n",
    "\n",
    "#Make another copy to perform further preprocessing\n",
    "d.clean.preprocess = d.clean.NoNa.copy()\n",
    "\n",
    "#Normalize continuous features\n",
    "d.clean.preprocess['Age'] = (d.clean.preprocess['Age'] - d.clean.preprocess['Age'].mean())/d.clean.preprocess['Age'].std()\n",
    "d.clean.preprocess['Compa_Ratio'] = (d.clean.preprocess['Compa_Ratio'] - d.clean.preprocess['Compa_Ratio'].mean())/d.clean.preprocess['Compa_Ratio'].std()\n",
    "d.clean.preprocess['Count_of_Direct_Reports'] = (d.clean.preprocess['Count_of_Direct_Reports'] - d.clean.preprocess['Count_of_Direct_Reports'].mean())/d.clean.preprocess['Count_of_Direct_Reports'].std()\n",
    "d.clean.preprocess['Years_in_Service_(Continuous_Service_Date)'] = (d.clean.preprocess['Years_in_Service_(Continuous_Service_Date)'] - d.clean.preprocess['Years_in_Service_(Continuous_Service_Date)'].mean())/d.clean.preprocess['Years_in_Service_(Continuous_Service_Date)'].std()\n",
    "\n",
    "#Factorize categorical features with natural ordinal relationships\n",
    "d.clean.preprocess['Job_Level'] = pd.factorize(d.clean.preprocess['Job_Level'], sort = True)[0]\n",
    "\n",
    "d.clean.preprocess['Age_Buckets_Factorized'] = pd.factorize(d.clean.preprocess['Age_-_Buckets'], sort = True)[0]\n",
    "d.clean.preprocess.drop('Age_-_Buckets', axis = 1, inplace = True)\n",
    "\n",
    "Job_Group_Dict = {'Operators' : 0, 'Support' : 1, \n",
    "                  'Operations Supervisors' : 2, 'Professional' : 3, \n",
    "                  'Management' : 4, 'Executive' : 5}\n",
    "d.clean.preprocess['Job_Group_Factorized'] = d.clean.preprocess['Job_Group'].map(Job_Group_Dict)\n",
    "d.clean.preprocess.drop('Job_Group', axis = 1, inplace = True)\n",
    "\n",
    "#One-hot-encode the rest of the categorical features - only run this for the subset being used\n",
    "d.clean.preprocess = pd.get_dummies(d.clean.preprocess, columns = ['Job_Category_(Job_Class)', 'Gender', 'Region'\n",
    "                                             , 'Country', 'Management_Level_(Picklist_Label)',\n",
    "                                             'Compensation_Grade', 'Marital_Status', 'Location', \n",
    "                                             'Ethnicity'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ddea26",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This data had employees all over the world and therefore \n",
    "salary info from many different countries.\n",
    "Here we read in a dataframe containing exchange rates and make these conversions\n",
    "(up to date from the day of building the model).\n",
    "'''\n",
    "\n",
    "#Read in conversions\n",
    "exchange = pd.read_csv('Exchange Rates.csv')\n",
    "\n",
    "#Create a new column that converts the \"midpoint\" feature to USD\n",
    "d['Midpoint_USD'] = None\n",
    "for row in range(1,d.shape[0]):\n",
    "    if not pd.isna(d['Currency'][row]):\n",
    "        country = d['Currency'][row]\n",
    "        d['Midpoint_USD'][row] = d['Annualized_Mid_Point'][row]/exchange[exchange['Currency'] == country].iloc[0][1]\n",
    "    \n",
    "#Create a new column that calculates salary based on USD Midpoint and compa ratio\n",
    "d['Salary'] = None\n",
    "for row in range(1,d.shape[0]):\n",
    "    if not pd.isna(d['Midpoint_USD'][row]):\n",
    "        d['Salary'][row] = d['Midpoint_USD'][row]*d['Compa_Ratio'][row]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5116d1c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read in data provided by company to map the cost of employee churn by compensation grade\n",
    "churnCost = pd.read_csv('CompansationGrade_cost_map.csv')\n",
    "\n",
    "#Add \"cost if churn\" column using salary and churn cost\n",
    "d.clean['Cost_If_Churn'] = None\n",
    "for row in range(d.clean.shape[0]):\n",
    "    if (not pd.isna(d.clean['Salary'][row])) & (not pd.isna(d.clean['Compensation_Grade'][row])):\n",
    "        compa = d.clean['Compensation_Grade'][row]\n",
    "        d.clean['Cost_If_Churn'][row] = d.clean['Salary'][row]*churnCost[churnCost['Compensation Grade'] == compa].iloc[0][1] \n",
    "\n",
    "#Create final dataframe containing cost if churn and GID_Anonymized\n",
    "cost_df = d.clean.copy()\n",
    "cost_df.reset_index(inplace = True)\n",
    "cost_df = cost_df[['GID_anonymized', 'Cost_If_Churn']]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
